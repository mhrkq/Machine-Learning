{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81689,"databundleVersionId":8855727,"sourceType":"competition"},{"sourceId":3937548,"sourceType":"datasetVersion","datasetId":2337399}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# PCA (not a sklearn logistic regression package)\n# because dataset has 5000 features, use PCA(Principal Component Analysis) to reduce the number of features to 2 to form the graph\n\ndef load_data(file_path):\n    data = pd.read_csv(file_path)\n    X = data.iloc[:, 2:].values  # Features (column 3 to last column)\n    y = data.iloc[:, 1].values   # Target variable (y is the second column)\n    return X, y\n\ndef sigmoid(z):\n    return 1.0 / (1 + np.exp(-z))\n\n# LOSS FUNCTION\n# RuntimeWarning: divide by zero encountered in log\n#  return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n# when np.log takes in value of 0 (if y_hat = 0 or 1) \n# the logarithm of 0 is undefined in the real numbers, which will lead to this warning.\n\n# RuntimeWarning: invalid value encountered in multiply\n#  return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n# when np.log takes in negative value (y_hat < 0 or y_hat > 1)\n# this will result in NaN (Not a Number) values in the calculation.\ndef loss(y, y_hat):\n    eps = 1e-8  # Small epsilon to prevent log(0)\n    y_hat = np.clip(y_hat, eps, 1 - eps)  # Clip y_hat to (eps, 1-eps) range\n    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n\ndef gradients(X, y, y_hat):\n    m = X.shape[0]\n    dw = (1/m) * np.dot(X.T, (y_hat - y))\n    db = (1/m) * np.sum(y_hat - y)\n    return dw, db\n\n# NORMALISE FUNCTION\n# RuntimeWarning: invalid value encountered in divide occurs when there are elements in X.std(axis=0) that are 0\n# if all values in a column of X are same, will result in a standard deviation of 0\n# create a boolean mask zero_std_mask to identify columns where the standard deviation is 0\n# handle columns with zero standard deviation separately to avoid division by 0 (subtract the mean (X_mean) directly)\n# for columns with non-zero standard deviation, normalise as usual\ndef normalise(X):\n    # calculate mean and standard deviation\n    X_mean = X.mean(axis=0)\n    X_std = X.std(axis=0)\n    \n    # check for zero standard deviation\n    zero_std_mask = X_std == 0\n    \n    # avoid division by zero and normalise\n    X_normalised = np.zeros_like(X)\n    X_normalised[:, ~zero_std_mask] = (X[:, ~zero_std_mask] - X_mean[~zero_std_mask]) / X_std[~zero_std_mask]\n    X_normalised[:, zero_std_mask] = X[:, zero_std_mask] - X_mean[zero_std_mask]\n    \n    return X_normalised\n\ndef train(X, y, bs, epochs, lr):\n    m, n = X.shape\n    w = np.zeros((n, 1))\n    b = 0\n    y = y.reshape(m, 1)\n    X = normalise(X)\n    losses = []\n\n    for epoch in range(epochs):\n        for i in range((m-1) // bs + 1):\n            start_i = i * bs\n            end_i = start_i + bs\n            xb = X[start_i:end_i]\n            yb = y[start_i:end_i]\n\n            y_hat = sigmoid(np.dot(xb, w) + b)\n            dw, db = gradients(xb, yb, y_hat)\n            w -= lr * dw\n            b -= lr * db\n\n        l = loss(y, sigmoid(np.dot(X, w) + b))\n        losses.append(l)\n        if epoch % 100 == 0:\n            print(f'Epoch {epoch}, Loss: {l}')\n\n    return w, b, losses\n\ndef predict(X, w, b):\n    X = normalise(X)\n    preds = sigmoid(np.dot(X, w) + b)\n    return np.array([1 if i > 0.5 else 0 for i in preds])\n\ndef accuracy(y, y_hat):\n    return np.sum(y == y_hat) / len(y)\n\ndef plot_decision_boundary(X, y, w, b):\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n    \n    x1 = [min(X_pca[:, 0]), max(X_pca[:, 0])]\n    m = -w[0]/w[1]\n    c = -b/w[1]\n    x2 = m * np.array(x1) + c\n    \n    plt.figure(figsize=(10, 8))\n    plt.plot(X_pca[:, 0][y == 0], X_pca[:, 1][y == 0], \"g^\")\n    plt.plot(X_pca[:, 0][y == 1], X_pca[:, 1][y == 1], \"bs\")\n    plt.xlim([min(X_pca[:, 0]) - 1, max(X_pca[:, 0]) + 1])\n    plt.ylim([min(X_pca[:, 1]) - 1, max(X_pca[:, 1]) + 1])\n    plt.xlabel(\"Principal Component 1\")\n    plt.ylabel(\"Principal Component 2\")\n    plt.title('Decision Boundary')\n    plt.plot(x1, x2, 'y-')\n    plt.show()\n\n# load and process the dataset\nfile_path = \"/kaggle/input/mlprojectdataset/train_tfidf_features.csv\"\nX, y = load_data(file_path)\n\n# train the model\nbatch_size = 100\nepochs = 1000\nlearning_rate = 0.01\n\nw, b, losses = train(X, y, bs=batch_size, epochs=epochs, lr=learning_rate)\ny_pred = predict(X, w, b)\nacc = accuracy(y, y_pred)\n\nprint(f'Weights: {w}')\nprint(f'Bias: {b}')\nprint(f'Accuracy: {acc}')\n\n# plot the decision boundary\nplot_decision_boundary(X, y, w, b)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-02T15:01:51.506304Z","iopub.execute_input":"2024-07-02T15:01:51.506652Z"},"trusted":true},"execution_count":null,"outputs":[]}]}